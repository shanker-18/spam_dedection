{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Spam Email Classification\n",
    "\n",
    "Objective: Build a model to classify emails as spam or not spam (ham) based on the email content.\n",
    "\n",
    "Problem Type: Natural Language Processing (NLP)"
   ],
   "id": "799b0b2c8bd465bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:43.214435Z",
     "start_time": "2024-12-11T10:04:43.174909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv(r\"C:\\Users\\admin\\Downloads\\spam_email_data.csv\")\n",
    "print(data.head())"
   ],
   "id": "3d6ce6136b2d46bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Email ID             Sender                       Subject  \\\n",
      "0       E1  carol@example.com             Win a free iPhone   \n",
      "1       E2  alice@example.com            Limited time offer   \n",
      "2       E3    bob@example.com             Win a free iPhone   \n",
      "3       E4   dave@example.com  Your account needs attention   \n",
      "4       E5  alice@example.com             Win a free iPhone   \n",
      "\n",
      "                          Email Content  Is Spam  \n",
      "0                   Update your profile        0  \n",
      "1                   Update your profile        0  \n",
      "2         Exclusive offer just for you!        0  \n",
      "3  Your bank account needs verification        0  \n",
      "4                   Update your profile        0  \n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:48.499220Z",
     "start_time": "2024-12-11T10:04:48.375915Z"
    }
   },
   "cell_type": "code",
   "source": "data.info()",
   "id": "274250f7fbf4abcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Email ID       100 non-null    object\n",
      " 1   Sender         100 non-null    object\n",
      " 2   Subject        100 non-null    object\n",
      " 3   Email Content  100 non-null    object\n",
      " 4   Is Spam        100 non-null    int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 4.0+ KB\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:51.371272Z",
     "start_time": "2024-12-11T10:04:51.286622Z"
    }
   },
   "cell_type": "code",
   "source": "data.describe()",
   "id": "8a74603434980967",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          Is Spam\n",
       "count  100.000000\n",
       "mean     0.160000\n",
       "std      0.368453\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      0.000000\n",
       "75%      0.000000\n",
       "max      1.000000"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Is Spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.368453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:55.281838Z",
     "start_time": "2024-12-11T10:04:55.253843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n"
   ],
   "id": "2955459168e318b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1.Data Preprocessing\n",
    "\n",
    "Clean email"
   ],
   "id": "5e6277d6218d8f9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:04:59.119610Z",
     "start_time": "2024-12-11T10:04:59.072609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "data['Cleaned Content'] = data['Sender'].apply(lambda text: ' '.join(\n",
    "    [word for word in re.sub(r'[^a-z\\s]', '', text.lower()).split() if word not in stop_words]\n",
    "))\n",
    "print(data[['Sender', 'Cleaned Content']].head())"
   ],
   "id": "59e78fd03bf480f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sender  Cleaned Content\n",
      "0  carol@example.com  carolexamplecom\n",
      "1  alice@example.com  aliceexamplecom\n",
      "2    bob@example.com    bobexamplecom\n",
      "3   dave@example.com   daveexamplecom\n",
      "4  alice@example.com  aliceexamplecom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tokenize the text into words",
   "id": "db8820da4e05dfcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:05:03.240600Z",
     "start_time": "2024-12-11T10:05:03.109060Z"
    }
   },
   "cell_type": "code",
   "source": "nltk.download('punkt_tab')",
   "id": "81dcfa157d6b2a85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:05:05.669779Z",
     "start_time": "2024-12-11T10:05:05.455772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "data['Sentences'] = data['Sender'].apply(sent_tokenize)\n",
    "data['Words'] = data['Sender'].apply(word_tokenize)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "data['Cleaned Content'] = data['Sender'].apply(preprocess_text)\n",
    "print(data[['Sender', 'Sentences', 'Words', 'Cleaned Content']].head())\n"
   ],
   "id": "f3caf57b7304ff0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Sender            Sentences                    Words  \\\n",
      "0  carol@example.com  [carol@example.com]  [carol, @, example.com]   \n",
      "1  alice@example.com  [alice@example.com]  [alice, @, example.com]   \n",
      "2    bob@example.com    [bob@example.com]    [bob, @, example.com]   \n",
      "3   dave@example.com   [dave@example.com]   [dave, @, example.com]   \n",
      "4  alice@example.com  [alice@example.com]  [alice, @, example.com]   \n",
      "\n",
      "   Cleaned Content  \n",
      "0  carolexamplecom  \n",
      "1  aliceexamplecom  \n",
      "2    bobexamplecom  \n",
      "3   daveexamplecom  \n",
      "4  aliceexamplecom  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Feature Engineering:\n",
    "\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency)"
   ],
   "id": "731c74ed5f6a10fa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:05:11.243132Z",
     "start_time": "2024-12-11T10:05:11.208138Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000) \n",
    "X = vectorizer.fit_transform(data['Cleaned Content']).toarray()\n",
    "y = data['Is Spam'] \n",
    "print(\"Shape of the feature matrix:\", X.shape)"
   ],
   "id": "360ba531aede91f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the feature matrix: (100, 4)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model Training:\n",
    "\n",
    "Naive Bayes classifier"
   ],
   "id": "eac985ce3f491682"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:05:18.742503Z",
     "start_time": "2024-12-11T10:05:18.671940Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "data.columns = data.columns.str.strip()\n",
    "X = data['Email Content']\n",
    "y = data['Is Spam']\n",
    "stop_words = set(stopwords.words('english'))\n",
    "X_cleaned = X.apply(lambda text: ' '.join(\n",
    "    [word for word in re.sub(r'[^a-z\\s]', '', text.lower()).split() if word not in stop_words]\n",
    "))\n",
    "vectorizer = CountVectorizer()\n",
    "X_vectorized = vectorizer.fit_transform(X_cleaned)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Model training complete. Predictions made on the test set.\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))"
   ],
   "id": "25830bd75e875e63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete. Predictions made on the test set.\n",
      "Accuracy: 0.8\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      1.00      0.89        16\n",
      "           1       1.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.80        20\n",
      "   macro avg       0.90      0.50      0.44        20\n",
      "weighted avg       0.84      0.80      0.71        20\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Evaluation:\n",
    "\n",
    "Accuracy\n",
    "\n",
    "Precision\n",
    "\n",
    "Recall\n",
    "\n",
    "F1 Score"
   ],
   "id": "2e50b29f3c3bfe56"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:05:29.509437Z",
     "start_time": "2024-12-11T10:05:29.452877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, pos_label=1, zero_division=1)  \n",
    "recall = recall_score(y_test, y_pred, pos_label=1, zero_division=1)\n",
    "f1 = f1_score(y_test, y_pred, pos_label=1, zero_division=1)\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")"
   ],
   "id": "10cb4ec00a4c6cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "Accuracy: 0.80\n",
      "Precision: 1.00\n",
      "Recall: 0.00\n",
      "F1 Score: 0.00\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
